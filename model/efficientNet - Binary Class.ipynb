{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a88560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import * #Efficient Net included here\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eea4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5b201",
   "metadata": {},
   "source": [
    "### Solve as Binary Problem - 2 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91eb794",
   "metadata": {},
   "source": [
    "#### Get Dataframe that contain image-name, class-name, and class-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9817c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '../data/manual-clusters/20220430/categorized/'\n",
    "\n",
    "newlabels = []\n",
    "\n",
    "for label in os.listdir(datafolder):\n",
    "    for image in os.listdir(datafolder + label):\n",
    "        newlabels.append({\n",
    "            'filename': datafolder + label + \"/\" + image,\n",
    "            'classname': label,\n",
    "            \n",
    "        })\n",
    "        \n",
    "newlabels = pd.DataFrame(newlabels)\n",
    "newlabels['class_id'] = newlabels['classname']\n",
    "newlabels.replace({\"class_id\":{\"01\":0, \"02-round-end\":0,\"03-mib\":0,\"04\":1,\"05-hole-flat\":0,\n",
    "                               \"06\":0,\"07-honeycomb\":0,\"08\":0,\"09\":0,\"10-honeycombhollow\":0,\n",
    "                               \"11-longthin\":0}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99e109b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    516\n",
       "1    417\n",
       "Name: class_id, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlabels['class_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e3aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up image directory\n",
    "TRAIN_IMAGES_PATH = '../data/efNet_data_binary/images/train'\n",
    "VAL_IMAGES_PATH = '../data/efNet_data_binary/images/val'\n",
    "os.makedirs(TRAIN_IMAGES_PATH, exist_ok = True)\n",
    "os.makedirs(VAL_IMAGES_PATH, exist_ok = True)\n",
    "\n",
    "classes = set(newlabels['class_id'])\n",
    "# Create directories for each class.\n",
    "for class_id in [x for x in range(len(classes))]:\n",
    "    os.makedirs(os.path.join(TRAIN_IMAGES_PATH, str(class_id)), exist_ok = True)\n",
    "    os.makedirs(os.path.join(VAL_IMAGES_PATH, str(class_id)), exist_ok = True)\n",
    "    \n",
    "def preproccess_data(df, images_path):\n",
    "    for column, row in df.iterrows():\n",
    "        class_id = row['class_id']\n",
    "        shutil.copy(row['filename'], os.path.join(images_path, str(class_id)))\n",
    "        \n",
    "#Split the dataset into 80% training and 20% validation\n",
    "df_train, df_valid = model_selection.train_test_split(newlabels, test_size=0.2, random_state=42, shuffle=True)\n",
    "#run the  function on each of them\n",
    "preproccess_data(df_train, TRAIN_IMAGES_PATH)\n",
    "preproccess_data(df_valid, VAL_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0f045",
   "metadata": {},
   "source": [
    "#### Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03205c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "conv_base = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "NUMBER_OF_CLASSES = 2\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
    "#avoid overfitting\n",
    "model.add(layers.Dropout(rate=0.2, name=\"dropout_out\"))\n",
    "# Set NUMBER_OF_CLASSES to the number of your final predictions.\n",
    "model.add(layers.Dense(NUMBER_OF_CLASSES, activation=\"softmax\", name=\"fc_out\"))\n",
    "#conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eeb2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 746 images belonging to 2 classes.\n",
      "Found 187 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "height,width=224,224\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    # This is the target directory\n",
    "    TRAIN_IMAGES_PATH,\n",
    "    # All images will be resized to target height and width.\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    # Since we use categorical_crossentropy loss, we need categorical labels\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_IMAGES_PATH,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
    "    metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=2), \"acc\"], #'acc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a395771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 126s 3s/step - loss: 3.3916 - top_k_categorical_accuracy: 1.0000 - acc: 0.4822 - val_loss: 4.0937 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5455\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 126s 3s/step - loss: 1.8431 - top_k_categorical_accuracy: 1.0000 - acc: 0.5767 - val_loss: 4.2051 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5398\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 131s 3s/step - loss: 1.6812 - top_k_categorical_accuracy: 1.0000 - acc: 0.6329 - val_loss: 4.9732 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5568\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 132s 3s/step - loss: 1.6151 - top_k_categorical_accuracy: 1.0000 - acc: 0.6467 - val_loss: 4.0845 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5455\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 131s 3s/step - loss: 1.3800 - top_k_categorical_accuracy: 1.0000 - acc: 0.6699 - val_loss: 2.4924 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5398\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 129s 3s/step - loss: 1.3416 - top_k_categorical_accuracy: 1.0000 - acc: 0.6863 - val_loss: 2.0220 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5455\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 129s 3s/step - loss: 1.4571 - top_k_categorical_accuracy: 1.0000 - acc: 0.6384 - val_loss: 2.5297 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5170\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 127s 3s/step - loss: 1.2886 - top_k_categorical_accuracy: 1.0000 - acc: 0.6726 - val_loss: 2.3165 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.5057\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 126s 3s/step - loss: 1.2594 - top_k_categorical_accuracy: 1.0000 - acc: 0.6904 - val_loss: 2.9454 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.4886\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 125s 3s/step - loss: 1.3296 - top_k_categorical_accuracy: 1.0000 - acc: 0.6712 - val_loss: 2.6031 - val_top_k_categorical_accuracy: 1.0000 - val_acc: 0.4489\n",
      "Wall time: 21min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUMBER_OF_TRAINING_IMAGES = 746\n",
    "NUMBER_OF_VALIDATION_IMAGES = 187\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=NUMBER_OF_TRAINING_IMAGES // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=NUMBER_OF_VALIDATION_IMAGES // batch_size,\n",
    "    verbose=1,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d378f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./efficientNet_binary\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./efficientNet_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e992402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63010091",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ff5c0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 103, 1: 84})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "collections.Counter(validation_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99acbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.83      0.66       103\n",
      "           1       0.47      0.19      0.27        84\n",
      "\n",
      "    accuracy                           0.54       187\n",
      "   macro avg       0.51      0.51      0.47       187\n",
      "weighted avg       0.52      0.54      0.49       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Classification Report')\n",
    "target_names = [\"0\",\"1\"]\n",
    "print(classification_report(validation_generator.classes, y_pred_class, target_names=target_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d30214f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(validation_generator.classes, y_pred_class)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a0ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 6s 492ms/step - loss: 2.5646 - top_k_categorical_accuracy: 0.5455 - acc: 0.4064\n",
      "evaluate top_k_categorical_accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(validation_generator, verbose=1)\n",
    "print(\"%s%s: %.2f%%\" % (\"evaluate \",model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
