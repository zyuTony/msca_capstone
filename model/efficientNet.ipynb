{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770c7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# image\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Utilities\n",
    "from keras.preprocessing.image import load_img, img_to_array, save_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Pre-trained model\n",
    "from tensorflow.keras.applications import resnet50, densenet\n",
    "\n",
    "# Architectures\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Optimizers, metrics, initializers\n",
    "from tensorflow.keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a88560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import * #Efficient Net included here\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eea4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91eb794",
   "metadata": {},
   "source": [
    "#### Get Dataframe that contain image-name, class-name, and class-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e3aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '../data/manual-clusters/20220430/categorized/'\n",
    "\n",
    "newlabels = []\n",
    "\n",
    "for label in os.listdir(datafolder):\n",
    "    for image in os.listdir(datafolder + label):\n",
    "        newlabels.append({\n",
    "            'filename': datafolder + label + \"/\" + image,\n",
    "            'classname': label,\n",
    "            \n",
    "        })\n",
    "        \n",
    "newlabels = pd.DataFrame(newlabels)\n",
    "newlabels['class_id'] = newlabels['classname']\n",
    "newlabels.replace({\"class_id\":{\"01\":0, \"02-round-end\":1,\"03-mib\":2,\"04\":3,\"05-hole-flat\":4,\n",
    "                               \"06\":5,\"07-honeycomb\":6,\"08\":7,\"09\":8,\"10-honeycombhollow\":9,\n",
    "                               \"11-longthin\":10}}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79886c",
   "metadata": {},
   "source": [
    "#### Prepare the dataset. Get data into folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92be58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = '../efNet/images/train'\n",
    "VAL_IMAGES_PATH = '../efNet/images/val'\n",
    "os.makedirs(TRAIN_IMAGES_PATH, exist_ok = True)\n",
    "os.makedirs(VAL_IMAGES_PATH, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7e0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(newlabels['class_id'])\n",
    "# Create directories for each class.\n",
    "for class_id in [x for x in range(len(classes))]:\n",
    "    os.makedirs(os.path.join(TRAIN_IMAGES_PATH, str(class_id)), exist_ok = True)\n",
    "    os.makedirs(os.path.join(VAL_IMAGES_PATH, str(class_id)), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36d5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccess_data(df, images_path):\n",
    "    for column, row in df.iterrows():\n",
    "        class_id = row['class_id']\n",
    "        shutil.copy(row['filename'], os.path.join(images_path, str(class_id)))\n",
    "        \n",
    "#Split the dataset into 80% training and 20% validation\n",
    "df_train, df_valid = model_selection.train_test_split(newlabels, test_size=0.2, random_state=42, shuffle=True)\n",
    "#run the  function on each of them\n",
    "preproccess_data(df_train, TRAIN_IMAGES_PATH)\n",
    "preproccess_data(df_valid, VAL_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0f045",
   "metadata": {},
   "source": [
    "#### augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0c6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "conv_base = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b03205c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 11\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
    "#avoid overfitting\n",
    "model.add(layers.Dropout(rate=0.2, name=\"dropout_out\"))\n",
    "# Set NUMBER_OF_CLASSES to the number of your final predictions.\n",
    "model.add(layers.Dense(NUMBER_OF_CLASSES, activation=\"softmax\", name=\"fc_out\"))\n",
    "#conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eeb2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 746 images belonging to 11 classes.\n",
      "Found 187 images belonging to 11 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zongy\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "height,width=224,224\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    # This is the target directory\n",
    "    TRAIN_IMAGES_PATH,\n",
    "    # All images will be resized to target height and width.\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    # Since we use categorical_crossentropy loss, we need categorical labels\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_IMAGES_PATH,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "    metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=2), \"acc\"], #'acc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a395771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 132s 3s/step - loss: 7.6133 - top_k_categorical_accuracy: 0.1781 - acc: 0.0863 - val_loss: 7.9812 - val_top_k_categorical_accuracy: 0.0682 - val_acc: 0.0227\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 131s 3s/step - loss: 4.3017 - top_k_categorical_accuracy: 0.3904 - acc: 0.2384 - val_loss: 5.9408 - val_top_k_categorical_accuracy: 0.0739 - val_acc: 0.0284\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 120s 3s/step - loss: 4.1454 - top_k_categorical_accuracy: 0.4658 - acc: 0.3164 - val_loss: 3.9044 - val_top_k_categorical_accuracy: 0.1932 - val_acc: 0.0284\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 119s 3s/step - loss: 3.5398 - top_k_categorical_accuracy: 0.5151 - acc: 0.3603 - val_loss: 3.3359 - val_top_k_categorical_accuracy: 0.2670 - val_acc: 0.1591\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 119s 3s/step - loss: 3.2419 - top_k_categorical_accuracy: 0.5707 - acc: 0.3995 - val_loss: 3.7554 - val_top_k_categorical_accuracy: 0.2330 - val_acc: 0.1648\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 117s 3s/step - loss: 3.3777 - top_k_categorical_accuracy: 0.5370 - acc: 0.3959 - val_loss: 3.6812 - val_top_k_categorical_accuracy: 0.1534 - val_acc: 0.0852\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 122s 3s/step - loss: 3.2272 - top_k_categorical_accuracy: 0.5836 - acc: 0.4151 - val_loss: 4.1011 - val_top_k_categorical_accuracy: 0.1136 - val_acc: 0.0739\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 118s 3s/step - loss: 3.0092 - top_k_categorical_accuracy: 0.6000 - acc: 0.4178 - val_loss: 3.8068 - val_top_k_categorical_accuracy: 0.1875 - val_acc: 0.0739\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 120s 3s/step - loss: 2.8578 - top_k_categorical_accuracy: 0.6068 - acc: 0.4425 - val_loss: 2.8028 - val_top_k_categorical_accuracy: 0.3239 - val_acc: 0.1534\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 126s 3s/step - loss: 2.8272 - top_k_categorical_accuracy: 0.6000 - acc: 0.4342 - val_loss: 2.5574 - val_top_k_categorical_accuracy: 0.5511 - val_acc: 0.4205\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_TRAINING_IMAGES = 746\n",
    "NUMBER_OF_VALIDATION_IMAGES = 187\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=NUMBER_OF_TRAINING_IMAGES // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=NUMBER_OF_VALIDATION_IMAGES // batch_size,\n",
    "    verbose=1,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a0ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 6s 492ms/step - loss: 2.5646 - top_k_categorical_accuracy: 0.5455 - acc: 0.4064\n",
      "evaluate top_k_categorical_accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(validation_generator, verbose=1)\n",
    "print(\"%s%s: %.2f%%\" % (\"evaluate \",model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
